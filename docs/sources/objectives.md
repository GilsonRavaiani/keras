
## Usage of objectives

An objective function (or loss function, or optimization score function) is one of the two parameters required to compile a model:

```python
model.compile(loss='mean_squared_error', optimizer='sgd')
```

You can either pass the name of an existing objective, or pass a Theano symbolic function that returns a scalar for each data-point and takes the following two arguments:

- __y_true__: True labels. Theano tensor.
- __y_pred__: Predictions. Theano tensor of the same shape as y_true.
- __sample_weight__: Relative value of how much a given sample affects the cost function
- __mask__: Binary mask value. Used when dealing recurrent model and different lenght inputs.

The actual optimized objective is the mean of the output array across all datapoints.

For a few examples of such functions, check out the [objectives source](https://github.com/fchollet/keras/blob/master/keras/objectives.py).

## Available objectives

- __mean_squared_error__ / __mse__
- __mean_absolute_error__ / __mae__
- __mean_absolute_percentage_error__ / __mape__
- __mean_squared_logarithmic_error__ / __msle__
- __squared_hinge__
- __hinge__
- __binary_crossentropy__: Also known as logloss.
- __categorical_crossentropy__: Also known as multiclass logloss. __Note__: using this objective requires that your labels are binary arrays of shape `(nb_samples, nb_classes)`.

## Writing new objectives

When you want to create your own objective function, it has to accept 4 inputs:

```python
def my_objective(y_true, y_pred, sample_weight, mask):
    return ((y_true - y_pred)**2).mean()
```

As you can see, the actual cost is computed comparing `y_true` to `y_pred`, but a compiled `model.train_on_batch`
accepts an input called `sample_weight` that can change how much each sample
contributes to the cost function and consequently to the gradients:
```python
model.train_on_batch(input, output, sample_weight=sw)
```

You can use this to reweight your objective accordingly.
```python
def my_objective(y_true, y_pred, sample_weight, mask):
    cost = (y_true - y_pred)**2
    cost = T.switch(T.eq(sample_weight, 0.), 0., cost)  # makes sure 0 * inf == 0, not NaN
    weighted_cost = cost * sample_weight
    return weighted_cost.sum() / sample_weight.sum()
```

The last input parameter `mask` is similar to `sample_weight`, but instead of
being provided by the user, it is generated by subclasses of
`keras.layers.MaskedLayer`. For example, when the input are variable length
time series, as those used to train recurrent neural networks. Your cost
function would look something like this:

```python
def my_objective(y_true, y_pred, sample_weight, mask):
    cost = (y_true - y_pred)**2
    cost = T.switch(T.eq(sample_weight, 0.), 0., cost)  # makes sure 0 * inf == 0, not NaN
    weighted_cost = cost * sample_weight
    if mask is None
        return weighted_cost.sum() / sample_weight.sum()
    else:
        mask = T.switch(T.eq(sample_weight, 0.), 0., mask)
        masked_cost = T.switch(T.eq(mask, 0.), 0., weighted_cost)  # Make sure we mask out that cost
        return masked_cost.sum() / (mask * sample_weight).sum()
```

Writing all those steps might be too repetitive, this is why Keras provides a
decorator to autmate all this for you. To get the same result as the cell
above, just use `objectivefy` as:
```python
from keras.objectives import objectivefy

@objectivefy
def my_objective(y_true, y_pred):
    return (y_true - y_pred)**2
```
