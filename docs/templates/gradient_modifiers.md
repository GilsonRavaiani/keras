## Usage of gradient_modifiers

`gradient_modifiers` allow for gradient modifications before weight updates within optimizers.

To clip gradients by l2 norm

```python
optimizer = Adam(gradient_modifier='l2_normalize')
```

Alternatively, multiple modifiers can be chained using

```python
from keras import gradient_modifiers

modifiers = [gradient_modifiers.ClipNorm(p=1), gradient_modifiers.ClipValue(clip_value=1.)]
optimizer = Adam(gradient_modifier=gradient_modifiers.CompositeModifier(modifiers))
```

## Available gradient modifiers


{{autogenerated}}

----
