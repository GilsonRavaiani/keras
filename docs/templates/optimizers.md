
## Usage of optimizers

An optimizer is one of the two arguments required for compiling a Keras model:

```python
from keras import optimizers

model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('softmax'))

sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)
```

You can either instantiate an optimizer before passing it to `model.compile()` , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used.

```python
# pass optimizer by name: default parameters will be used
model.compile(loss='mean_squared_error', optimizer='sgd')
```

---

## Usage of Tensorflow Optimizers

Apart from using optimizers implemented in Keras, you can also use optimizers shipped with Tensorflow:

```python
import tensorflow as tf

ADoptimizer = tf.train.AdadeltaOptimizer(learning_rate=1., rho=0.95, epsilon=1e-08)
model.compile(loss='mean_squared_error', optimizer=ADoptimizer)
```

This functionality also allows you to use optimizers from tf.contrib:

```python
adamwoptimizer = tf.contrib.opt.AdamWOptimizer(weight_decay=0.000001,learning_rate=0.01)
```

Note: Tensorflow optimizers do not have the same parameters as Keras optimizers, for details on Tensorflow optimizers and their parameters, please consult the [Tensorflow documentation](https://www.tensorflow.org/api_guides/python/train#Optimizers).

---

## Parameters common to all Keras optimizers

The parameters `clipnorm` and `clipvalue` can be used with all optimizers to control gradient clipping:

```python
from keras import optimizers

# All parameter gradients will be clipped to
# a maximum norm of 1.
sgd = optimizers.SGD(lr=0.01, clipnorm=1.)
```

```python
from keras import optimizers

# All parameter gradients will be clipped to
# a maximum value of 0.5 and
# a minimum value of -0.5.
sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)
```

---

{{autogenerated}}
